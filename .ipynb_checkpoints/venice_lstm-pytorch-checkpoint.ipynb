{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np# linear algebra\n",
    "from numpy import nan\n",
    "from numpy import array\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import os # accessing directory structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       wh  temp\n",
      "datetime                       \n",
      "2020-10-01 23:20:00  0.51  10.6\n",
      "2020-10-01 23:30:00  0.51  10.5\n",
      "2020-10-01 23:40:00  0.51  10.4\n",
      "2020-10-01 23:50:00  0.50  10.2\n",
      "2020-11-01 00:00:00  0.50  10.1\n"
     ]
    }
   ],
   "source": [
    "#already dropped NA's\n",
    "#sorted entries by date\n",
    "dataset = pd.read_csv('venice_new.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "\n",
    "print(dataset.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data \n",
    "https://machinelearningmastery.com/how-to-load-and-explore-household-electricity-usage-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset_19[month].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem framing\n",
    "I guess we want to for example predict the temperature and water level for the next month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "Classical linear methods include techniques are very effective for univariate time series forecasting. SARIMA\n",
    "or\n",
    "Deep Learning CNN LSTM and ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try DL LSTM of course \n",
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/\n",
    "#maybe group the data by day\n",
    "#then split the data into week chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_groups = dataset.resample('D')\n",
    "daily_data = daily_groups.mean()\n",
    "#print(daily_data.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even though there are only a few periods where there are daily measurements. I will interpolate the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the dataset based on previous/next values..\n",
    "#This works super good. it's like a gradual inference of the data points\n",
    "def impute_interpolate(dataset, col):\n",
    "    dataset[col] = dataset[col].interpolate()\n",
    "    # And fill the initial data points if needed:\n",
    "    dataset[col] = dataset[col].fillna(method='bfill')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= impute_interpolate(dataset=dataset, col='wh')\n",
    "data= impute_interpolate(dataset=data, col='temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data= data\n",
    "#print(daily_data.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection and Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so basically using the kalman diter didnt have much of an effect, at least based on v rough visual inspection.\n",
    "#Mostly smoothes values. Could have also used kalman instead of interpolation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now we just split it off. but actually we should take random weeks \n",
    "# dataset=data\n",
    "# training_frac=0.7\n",
    "# features=':'\n",
    "# end_training_set = int(training_frac * len(dataset.index))\n",
    "# training_set_X = dataset.ix[0:end_training_set]\n",
    "# training_set_y = dataset.ix[0:end_training_set]\n",
    "# test_set_X = dataset.ix[end_training_set:len(dataset.index)]\n",
    "# test_set_y = dataset.ix[end_training_set:len(dataset.index)]\n",
    "# print(len(test_set_X))\n",
    "# print(len(training_set_X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\tdata= data[:(int(len(data)/7))]\n",
    "\ttest_size= int(len(data)*0.3)\n",
    "\ttrain, test = data[1:-test_size], data[-test_size:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain= np.array(train) #if we dont convert here, it doesnt work or a list of dataframes comes out (without the second np.array)\n",
    "\ttest= np.array(test)\n",
    "\ttrain = np.array(np.split(train, len(train)/7)) # if we dont put the np.array here a list of numpy arrays comes out\n",
    "\ttest = np.array(np.split(test, len(test)/7))\n",
    "\treturn train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 7, 2)\n",
      "(81, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "train, test = split_dataset(data.values)\n",
    "# validate train data\n",
    "print(train.shape)\n",
    "# validate test\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weeks with forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_supervised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6a3f58d040ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#so we put in a week and try to get out a week\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_supervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'to_supervised' is not defined"
     ]
    }
   ],
   "source": [
    "#so we put in a week and try to get out a week\n",
    "data_X, data_y = to_supervised(train, n_input=7, n_out=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1569, 7, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this makes the sliding windows I think \n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doyle\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# univariate multi-step lstm\n",
    "from math import sqrt\n",
    " \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    " \n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 70, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    " \n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, 0]\n",
    "\t# reshape into [1, n_input, 1]\n",
    "\tinput_x = input_x.reshape((1, len(input_x), 1))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    " \n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm: [0.045] 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # load the new file\n",
    "# dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# # split into train and test\n",
    "# train, test = split_dataset(dataset.values)\n",
    "# # evaluate model and get scores\n",
    "n_input = 7\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044559534190630565 [0.01338532677584649, 0.027298564239541308, 0.032464001695585894, 0.03636929813585339, 0.05351009791776316, 0.059683588643199975, 0.06459402018051175]\n"
     ]
    }
   ],
   "source": [
    "print(score, scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I will try to do it in Pytorch\n",
    "https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014\n",
      "                       wh  temp\n",
      "datetime                       \n",
      "2019-01-11 00:00:00  0.45  16.4\n",
      "2019-01-11 00:10:00  0.46  16.4\n",
      "2019-01-11 00:20:00  0.47  16.3\n",
      "2019-01-11 00:30:00  0.48  16.3\n",
      "2019-01-11 00:40:00  0.49  16.2\n",
      "2019-01-11 00:50:00  0.49  16.2\n",
      "2019-01-11 01:00:00  0.49  16.2\n",
      "2019-01-11 01:10:00  0.49  16.2\n",
      "2019-01-11 01:20:00  0.50  16.2\n",
      "2019-01-11 01:30:00  0.50  16.2\n",
      "2019-01-11 01:40:00  0.49  16.2\n",
      "2019-01-11 01:50:00  0.49  16.1\n",
      "2019-01-11 02:00:00  0.49  16.2\n",
      "2019-01-11 02:10:00  0.48  16.1\n",
      "2019-01-11 02:20:00  0.48  16.1\n",
      "2019-01-11 02:30:00  0.48  16.2\n",
      "2019-01-11 02:40:00  0.48  16.2\n",
      "2019-01-11 02:50:00  0.47  16.1\n",
      "2019-01-11 03:00:00  0.45  16.1\n",
      "2019-01-11 03:10:00  0.44  16.1\n",
      "2019-01-11 03:20:00  0.43  16.1\n",
      "2019-01-11 03:30:00  0.41  16.1\n",
      "2019-01-11 03:40:00  0.40  16.1\n",
      "2019-01-11 03:50:00  0.39  16.1\n",
      "2019-01-11 04:00:00  0.37  16.1\n",
      "2019-01-11 04:10:00  0.36  16.1\n",
      "2019-01-11 04:20:00  0.35  16.0\n",
      "2019-01-11 04:30:00  0.34  16.0\n",
      "2019-01-11 04:40:00  0.33  16.0\n",
      "2019-01-11 04:50:00  0.33  16.0\n",
      "...                   ...   ...\n",
      "2019-12-13 21:20:00  0.71   9.4\n",
      "2019-12-13 21:30:00  0.73   9.5\n",
      "2019-12-13 21:40:00  0.74   9.5\n",
      "2019-12-13 21:50:00  0.77   9.5\n",
      "2019-12-13 22:00:00  0.80   9.5\n",
      "2019-12-13 22:10:00  0.81   9.4\n",
      "2019-12-13 22:20:00  0.83   9.4\n",
      "2019-12-13 22:30:00  0.82   9.4\n",
      "2019-12-13 22:40:00  0.82   9.4\n",
      "2019-12-13 22:50:00  0.82   9.5\n",
      "2019-12-13 23:00:00  0.81   9.5\n",
      "2019-12-13 23:10:00  0.79   9.5\n",
      "2019-12-13 23:20:00  0.77   9.4\n",
      "2019-12-13 23:30:00  0.75   9.4\n",
      "2019-12-13 23:40:00  0.73   9.4\n",
      "2019-12-13 23:50:00  0.71   9.4\n",
      "2019-12-14 00:00:00  0.69   9.3\n",
      "2019-12-14 00:10:00  0.67   9.3\n",
      "2019-12-14 00:20:00  0.65   9.3\n",
      "2019-12-14 00:30:00  0.64   9.3\n",
      "2019-12-14 00:40:00  0.63   9.3\n",
      "2019-12-14 00:50:00  0.62   9.2\n",
      "2019-12-14 01:00:00  0.62   9.2\n",
      "2019-12-14 01:10:00  0.62   9.1\n",
      "2019-12-14 01:20:00  0.62   9.1\n",
      "2019-12-14 01:30:00  0.62   9.1\n",
      "2019-12-14 01:40:00  0.62   9.2\n",
      "2019-12-14 01:50:00  0.61   9.2\n",
      "2019-12-14 02:00:00  0.61   9.2\n",
      "2019-12-14 02:10:00  0.61   9.3\n",
      "\n",
      "[9365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data_size = round(len(dataset)*0.3) \n",
    "print(test_data_size)\n",
    "\n",
    "train_data = dataset[:-test_data_size]\n",
    "test_data = dataset[-test_data_size:]\n",
    "\n",
    "#train_data= np.array(train_data)\n",
    "#test_data= np.array(test_data)\n",
    "\n",
    "train_data.shape\n",
    "type(train_data)\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9365, 2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to numpy and then to tensor\n",
    "\n",
    "tmp = train_data.values\n",
    "train_data = torch.from_numpy(tmp).float()\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensor \n",
    "#train_data_normalized = torch.FloatTensor(train).view(-1) #I think the flattenign was not necessart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4500, 16.4000],\n",
      "        [ 0.4600, 16.4000],\n",
      "        [ 0.4700, 16.3000],\n",
      "        [ 0.4800, 16.3000],\n",
      "        [ 0.4900, 16.2000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list of tuples\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]  # the label is sort o f the predicted. so all of of the predictions become one time longet now? \n",
    "        inout_seq.append((train_seq ,train_label))  #tuple containing the 7 days values and the next value \n",
    "    return inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window=7\n",
    "train_inout_seq = create_inout_sequences(train_data, train_window) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[0.6500, 9.3000],\n",
      "        [0.6400, 9.3000],\n",
      "        [0.6300, 9.3000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000]], dtype=torch.float64), tensor([[0.6200, 9.1000]], dtype=torch.float64)), (tensor([[0.6400, 9.3000],\n",
      "        [0.6300, 9.3000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000]], dtype=torch.float64), tensor([[0.6200, 9.2000]], dtype=torch.float64)), (tensor([[0.6300, 9.3000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.2000]], dtype=torch.float64), tensor([[0.6100, 9.2000]], dtype=torch.float64)), (tensor([[0.6200, 9.2000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6100, 9.2000]], dtype=torch.float64), tensor([[0.6100, 9.2000]], dtype=torch.float64)), (tensor([[0.6200, 9.2000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.1000],\n",
      "        [0.6200, 9.2000],\n",
      "        [0.6100, 9.2000],\n",
      "        [0.6100, 9.2000]], dtype=torch.float64), tensor([[0.6100, 9.3000]], dtype=torch.float64))]\n"
     ]
    }
   ],
   "source": [
    "print(train_inout_seq[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_layer_size=100, output_size=2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        input=input_seq.view(len(input_seq), 1,-1)\n",
    "        lstm_out, self.hidden_cell = self.lstm(input, self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq),-1))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doyle\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([7, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-aed397e55465>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msingle_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msingle_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \"\"\"\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "       # seq=seq.float()\n",
    "      #  labels= labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_pred = 7\n",
    "\n",
    "test_inputs = train_data_normalized[-train_window:].tolist()\n",
    "print(test_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
